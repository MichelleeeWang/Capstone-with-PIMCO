{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"4.Feature_Contribution_train.ipynb","provenance":[{"file_id":"1HY-CUICE-L5jjlmVq2YmgGsU1DI3b6cY","timestamp":1648679965565}],"machine_shape":"hm","collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"34hT5SyP417d","executionInfo":{"status":"ok","timestamp":1654739505508,"user_tz":240,"elapsed":5000,"user":{"displayName":"Hanqin Zhou","userId":"09777696392050403273"}},"outputId":"c554726d-801b-4c3c-9627-5d69e440bf3a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting treeinterpreter\n","  Downloading treeinterpreter-0.2.3-py2.py3-none-any.whl (6.0 kB)\n","Installing collected packages: treeinterpreter\n","Successfully installed treeinterpreter-0.2.3\n"]}],"source":["!pip install treeinterpreter"]},{"cell_type":"markdown","source":["In this notebook we use tree interpreter to calculate the feature contribution, that is, the contribution of each word in the sentence to the topic classifier. And then we find out the topic words in each sentence that contribute most to the classifier."],"metadata":{"id":"-68PDnK2wQFR"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from treeinterpreter import treeinterpreter as ti\n","\n","import re\n","import pickle\n","from nltk.corpus import stopwords\n","\n","# plotting\n","import seaborn as sns\n","from wordcloud import WordCloud\n","import matplotlib.pyplot as plt\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HpksaKeW43WW","executionInfo":{"status":"ok","timestamp":1654739543705,"user_tz":240,"elapsed":19305,"user":{"displayName":"Hanqin Zhou","userId":"09777696392050403273"}},"outputId":"eb8e6927-8d56-4447-e4d6-44c554e74169"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# download trained model from the previous section\n","data_file = '/content/drive/MyDrive/capstone-pimco/Part1/data/'\n","with open(data_file+'logi_clf.pickle', 'rb') as handle:\n","    logi_clf = pickle.load(handle)\n","with open(data_file+'rf.pickle', 'rb') as handle:\n","    rf = pickle.load(handle)\n","with open(data_file+'tfidf_vec.pickle', 'rb') as handle:\n","    tfidf_vec = pickle.load(handle)\n","with open(data_file+'tfidf_vec_is_growth.pickle', 'rb') as handle:\n","    tfidf_vec_is_growth = pickle.load(handle)"],"metadata":{"id":"q8jE4L8f6UEs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tfidf_vec.get_feature_names_out()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tjHZyxNw-Plq","executionInfo":{"status":"ok","timestamp":1654740191678,"user_tz":240,"elapsed":231,"user":{"displayName":"Hanqin Zhou","userId":"09777696392050403273"}},"outputId":"81a13197-228a-4172-acba-56b177c2f39b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['abating', 'abating economic', 'abroad', ..., 'year sell',\n","       'year still', 'year year'], dtype=object)"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["topics = ['credit', 'fed_funds_rate',\n","       'financial_markets', 'geopolitical_uncertainty',\n","       'growth', 'housing', 'inflation', 'labor_market',\n","       'liquidity_measures', 'quantitative_easing']"],"metadata":{"id":"7-UOaebfB7eA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["topic_dfs = {}\n","tfidf_Xs = {}\n","for topic in topics:\n","    topic_df = pd.read_excel('/content/drive/MyDrive/capstone-pimco/Part1/data/manual_direction_eng_completed/sampled_' + topic + '_direction.xlsx', usecols=[1, 2, 3, 4, 5, 6, 7])\n","    topic_df['topic'] = topic_df['topic'].astype('str')\n","    topic_dfs[topic] = topic_df\n","    tfidf_Xs[topic] = tfidf_vec.transform(topic_df['cleaned_text'])"],"metadata":{"id":"AqSBdBfk7Z_B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["classes = ['credit', 'fed funds rate',\n","       'financial markets', 'geopolitical uncertainty',\n","       'growth', 'housing', 'inflation', 'labor market',\n","       'liquidity measures', 'quantitative easing']"],"metadata":{"id":"mxTMNtfjEKEh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tqdm import tqdm"],"metadata":{"id":"w5IR7s4dohHf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["For each sentence, we first sort the words by their contributions in descending order. Then if the highest contribution is smaller than 0.1, we choose the first four words. If else, we choose all the words with contribution larger than 0.1."],"metadata":{"id":"2ikK3ZFJwliq"}},{"cell_type":"code","source":["for topic in topics:\n","    print(topic)\n","    topic_df = topic_dfs[topic]\n","    feature_contrbn = []\n","    _, _, contributions = ti.predict(rf, tfidf_Xs[topic])\n","    prediction = topic_df['topic']\n","    for i in tqdm(topic_df.index):\n","        pred = prediction[i]\n","        pred_idx = classes.index(pred)\n","        feature_list = []\n","        # sort the words by their contributions in descending order\n","        sorted_list = sorted(zip(contributions[i, :, pred_idx], tfidf_vec.get_feature_names_out()), \n","                                key=lambda x: -abs(x[0]))\n","        # If the highest contribution is smaller than 0.1, we choose the first four words.\n","        if sorted_list[0][0] < 0.1:\n","            feature_list.append((sorted_list[0][1], sorted_list[0][0]))\n","            feature_list.append((sorted_list[1][1], sorted_list[1][0]))\n","            feature_list.append((sorted_list[2][1], sorted_list[2][0]))\n","            feature_list.append((sorted_list[3][1], sorted_list[3][0]))\n","        # If else, we choose all the words with contribution larger than 0.1.\n","        else:\n","            for c, feature in sorted_list:\n","                if c >= 0.1:\n","                    feature_list.append((feature, c))\n","        feature_contrbn.append(feature_list)\n","    topic_dfs[topic]['feature_contribution'] = feature_contrbn\n","    topic_dfs[topic].to_csv(data_file+'feature_contribution_train/'+ topic + '.csv')"],"metadata":{"id":"QDhjaff29vmI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1654740239804,"user_tz":240,"elapsed":15439,"user":{"displayName":"Hanqin Zhou","userId":"09777696392050403273"}},"outputId":"67feebe1-7416-4720-bb02-209fd0dc82ba"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["credit\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 100/100 [00:00<00:00, 399.45it/s]\n"]},{"output_type":"stream","name":"stdout","text":["fed_funds_rate\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 100/100 [00:00<00:00, 387.37it/s]\n"]},{"output_type":"stream","name":"stdout","text":["financial_markets\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 100/100 [00:00<00:00, 435.13it/s]\n"]},{"output_type":"stream","name":"stdout","text":["geopolitical_uncertainty\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 100/100 [00:00<00:00, 457.99it/s]\n"]},{"output_type":"stream","name":"stdout","text":["growth\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 100/100 [00:00<00:00, 428.86it/s]\n"]},{"output_type":"stream","name":"stdout","text":["housing\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 100/100 [00:00<00:00, 450.99it/s]\n"]},{"output_type":"stream","name":"stdout","text":["inflation\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 100/100 [00:00<00:00, 440.82it/s]\n"]},{"output_type":"stream","name":"stdout","text":["labor_market\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 100/100 [00:00<00:00, 428.47it/s]\n"]},{"output_type":"stream","name":"stdout","text":["liquidity_measures\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 100/100 [00:00<00:00, 537.42it/s]\n"]},{"output_type":"stream","name":"stdout","text":["quantitative_easing\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 100/100 [00:00<00:00, 440.05it/s]\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"UUzMxPOWJ3YB"},"execution_count":null,"outputs":[]}]}